{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare Important Dates and News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import archive_urlparser\n",
    "import cnn_scrapper\n",
    "import breitbart_scrapper\n",
    "import fox_scrapper\n",
    "import msnbc_scrapper\n",
    "import os\n",
    "import text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dates():\n",
    "    #Input: None, assume that we're looking for all of 2016\n",
    "    #Output: A list of event dates\n",
    "    event_list = []\n",
    "    for i in range(1, 10):\n",
    "        month_string = '2016' + '0' + str(i)\n",
    "        for j in range(1, 10):\n",
    "            date_string = month_string + '0' + str(j)\n",
    "            event_list.append(date_string)\n",
    "        for j in range(10, 31):\n",
    "            date_string = month_string + str(j)\n",
    "            event_list.append(date_string)\n",
    "    for i in range(10, 13):\n",
    "        month_string = '2016' + str(i)\n",
    "        for j in range(1, 10):\n",
    "            date_string = month_string + '0' + str(j)\n",
    "            event_list.append(date_string)\n",
    "        for j in range(10, 31):\n",
    "            date_string = month_string + str(j)\n",
    "            event_list.append(date_string)\n",
    "    return event_list\n",
    "\n",
    "import os\n",
    "\n",
    "def make_folders(event_list, data_dir):\n",
    "    #Input: list of events and a output directory where each event in the list will have its own folder\n",
    "    #Output: New folders in the output directory\n",
    "    os.chdir(data_dir)\n",
    "    for event in event_list:\n",
    "        os.mkdir(data_dir+event)\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "event_dates = ['20160723', '20161007', '20160925', '20161004', '20161005', '20161019', '20161028']\n",
    "folders = ['breitbart', 'cnn', 'fox', 'msnbc']\n",
    "for folder in folders:\n",
    "    for date in event_list:\n",
    "        folder_date_text = open('/Volumes/articles/' + folder + '/' + 'raw/' + '_web_20160723153915_http/__www.cnn.com_2016_07_22_asia_mh370-pilot-simulation_index.html', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_dates = ['20160723', '20161007', '20160925', '20161004', '20161005', '20161019', '20161028']\n",
    "archive = \"https://web.archive.org\"\n",
    "url_list = ['www.msnbc.com/','http://www.breitbart.com/', \"http://www.cnn.com/\", \"http://www.msnbc.com/\"]\n",
    "scrapper_list = [msnbc_scrapper, breitbart_scrapper, cnn_scrapper, fox_scrapper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/articles/breitbart/raw/'\n",
    "events = event_list[51:100]\n",
    "make_folders(events, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_url = archive_urlparser.archive_parser()\n",
    "url = 'http://www.breitbart.com/'\n",
    "scrapper = breitbart_scrapper.breitbart_scrapper()\n",
    "event_list = create_dates()\n",
    "#event_dates = ['20160723', '20161007', '20160925', '20161004', '20161005', '20161019', '20161028']\n",
    "date_list = parsed_url.find_links(url, event_list[51:100])\n",
    "\n",
    "headline_dict = scrapper.find_headlines(date_list)\n",
    "raw_text = scrapper.headline_scrapper(headline_dict)\n",
    "\n",
    "for key_1 in raw_text.keys():\n",
    "    for key_2 in raw_text[key_1].keys():\n",
    "        text_file = open('/Volumes/articles/breitbart/raw/old_scrap/' + key_1[5:13] + \"/\" + key_2.replace(\"/\",\"_\"), 'w')\n",
    "        text_file.write(\"%s\" % raw_text[key_1][key_2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
