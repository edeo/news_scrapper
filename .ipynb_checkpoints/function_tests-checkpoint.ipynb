{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folders = ['breitbart', 'cnn', 'fox', 'msnbc']\n",
    "for folder in folders:\n",
    "    os.mkdir('/Volumes/articles/' + folder + '/raw')\n",
    "    os.mkdir('/Volumes/articles/' + folder + '/clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "class archive_parser(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def find_links(self, url, date_list):\n",
    "        \n",
    "        date_range_links = list()\n",
    "        archive = \"https://web.archive.org\"\n",
    "        \n",
    "        for day in date_list:\n",
    "            raw_html = requests.get(archive + \"/web/\" + day + \"000000\" + \"*/\" + url)\n",
    "            links = re.findall(\"/web/\"+ day + \".*/\" + url, raw_html.text)\n",
    "            try:\n",
    "                date_range_links.append(links[int(len(links)/2)])\n",
    "            except IndexError:\n",
    "                print raw_html\n",
    "            \n",
    "        return date_range_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archive = \"https://web.archive.org\"\n",
    "url = 'www.msnbc.com/'\n",
    "dates = ['20160715', '20160716', '20160718']\n",
    "test = archive_parser()\n",
    "date_list = test.find_links(url, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/web/20160715130636/http://www.msnbc.com/',\n",
       " u'/web/20160716132922/http://www.msnbc.com/',\n",
       " u'/web/20160718123750/http://www.msnbc.com/']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import msnbc_scrapper\n",
    "\n",
    "test = msnbc_scrapper.msnbc_scrapper()\n",
    "test_dict = test.find_headlines(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class HTML_cleaner(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def html_tags(self, url, date_list):\n",
    "        \n",
    "        date_range_links = list()\n",
    "        archive = \"https://web.archive.org\"\n",
    "        \n",
    "        for day in date_list:\n",
    "            raw_html = requests.get(archive + \"/web/\" + day + \"000000\" + \"*/\" + url)\n",
    "            links = re.findall(\"/web/\"+ day + \".*/\" + url, raw_html.text)\n",
    "            try:\n",
    "                date_range_links.append(links[int(len(links)/2)])\n",
    "            except IndexError:\n",
    "                print raw_html\n",
    "            \n",
    "        return date_range_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = test.headline_scrapper(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' Newt Gingrich bobbed and weaved on Friday, first suggesting that Muslim U.S. citizens could be deported for practicing Sharia law, only to later backtrack and blame the media for distorting his words.   Gingrich, a Republican once considered a top vice presidential pick, now insists that Muslim Americans can have faith that the U.S. Constitution will still protect them. But that\\'s little comfort for an embattled community that has noted the stunning frequency in which political leaders have said just the opposite.   The movement against Sharia law, an Islamic code of actions and beliefs woven into a legal framework, first spawned as a method to attack the institution of Islam but not individual people.   Related: Newt Gingrich Wants Tests for All U.S. Muslims After Nice Attack   Now that tactic is being flipped. Prominent conservative leaders are using Sharia to justify targeting individual Muslims, even to the point of stripping American citizens of their constitutional rights.   \"Use of anti-Muslim sentiment has been made mainstream and systematized in this election cycle,\" said Corey Saylor, director of anti-Islamophobia for the Council on American-Islamic Relations.   The specter of Sharia law as a threat to American well-being also came into play when U.S. courtrooms came across the issue \\u2014 for instance in domestic or child custody cases involving families from countries where Sharia law is observed.   Saylor said the true tipping point came in 2010 with a report called \"Sharia: The Threat for America,\" released by a conservative think tank called the Center for Security Policy. A number of former U.S. intelligence officers signed onto the report, arguing that Muslims had become radicalized and were attempting to subvert the U.S. through Sharia law.   The FBI later panned the report for overblowing the threats and coming to unsubstantiated conclusions. Still, it became the inspiration to a litany of legislation nationwide, with a handful of states enacting bans against Sharia law, including Alabama, Arizona, Kansas, Louisiana, North Carolina, South Dakota and Tennessee.   Related: Donald Trump\\'s Plan for a Muslim Database Draws Comparison to Nazi Germany   Prominent conservatives like Sarah Palin, Michelle Bachmann and even Gingrich were crusaders in popularizing the anti-Sharia movement. But over the last several years, the arguments in the movement have begun to shift. In fact, some are questioning whether Islam should even be categorized as a religion.   Because Sharia is a set of governing principles regulating both public and private life, critics suggest that Islam should be considered to be more of a political structure than a pure body of faith.   \"Islam is not a just a religion \\u2026 Islam is different,\" former Pennsylvania Sen. Rick Santorum said during a Republican debate last fall. Prominent conservative group Family Research Council claims that only \"16 percent of Islam is a religion.\" Ben Carson, another failed presidential candidate, echoed his remarks in January by asserting that Islam is not a religion, but rather, a \"life organization system.\" Carson also believes that a Muslim should not be president.   Taken a step further, as Santorum did during the presidential debate in December, then constitutional rights would not automatically apply to those who practice Sharia.   \"Islam is a religion, but it is also Sharia law, it is also a civil government, it is also a form of government. And, so, the idea that that is protected under the First Amendment is wrong,\" he said.   Conservative leaders who agree could use the argument to sidestep constitutional technicalities. But the use was often limited to justifying the federal government\\'s ability to monitor mosques and Muslim communities. That argument often arose after terror attacks were carried out by extremists.   The theory sets up this subtle narrative that Muslims, regardless of whether they adhere to Sharia law, are somehow un-American and therefore not protected by the Constitution. Gingrich got into trouble this week by stretching an already thin line of arguments to a breaking point and using Sharia law to justify deporting Muslims.   Presumptive Republican presidential nominee Donald Trump arguably champions the most egregious challenge to constitutional values in his proposal to ban all Muslims from coming to the U.S. He once toyed with the idea creating a Muslim database nationwide, but he eventually dropped the issue after it stirred up enough criticism and buzz.   Together, they raise doubts for some that Trump\\'s promise to put America first will include all Americans. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "test_case = raw_text['/web/20160716132922/http://www.msnbc.com/']['/web/20160716132922/http://www.nbcnews.com/politics/immigration/muslims-face-alarming-pattern-being-cast-un-american-n610436']\n",
    "\n",
    "soup = BeautifulSoup(test_case)\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "archive = \"https://web.archive.org\"\n",
    "link = '/web/20160715092329/http://money.cnn.com/2016/07/14/news/bank-atm-heist-taiwan/index.html'\n",
    "article_html = requests.get(archive + link)\n",
    "soup = BeautifulSoup(article_html.text, 'html.parser')\n",
    "content = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re \n",
    "\n",
    "class text_cleaner(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def clean_text_dict(self, raw_text_dict):\n",
    "        #This function takes a dictionary of a dictionary as an input\n",
    "        #This function performs the following operations:\n",
    "        #1. Escapes HTML characters\n",
    "        #3. Remove stop words\n",
    "        refined_text_dict = dict()\n",
    "        \n",
    "        for day in raw_text_dict:\n",
    "            for article in raw_text_dict[day]:\n",
    "                refined_text = list()\n",
    "                soup = BeautifulSoup(test_case)\n",
    "                html_strip = soup.text\n",
    "\n",
    "            refined_text_dict[day] = refined_text\n",
    "                \n",
    "        return refined_text_dict\n",
    "    \n",
    "    def clean_text_file(self, raw_text_file):\n",
    "        #This function takes an opened file as an input\n",
    "        #This function removes HTML/XML tags from the raw text files\n",
    "        #1. Escapes \n",
    "        #3. Remove stop words\n",
    "        html_tags = [\"\\\\xa0\", \"\\xa9\", \"\\\\u2019s\", \"\\\\u201d\",\"\\\\u201c\", '\\\\u2019', '\\\\u2018', '\\\\u2026', '\\\\u2014', '\\\\xfa']\n",
    "        refined_text = BeautifulSoup(raw_text_file, \"lxml\").text\n",
    "        refined_text = refined_text.replace(\"SIGN UP FOR OUR NEWSLETTER\", \" \")\n",
    "        \n",
    "        text = list(refined_text)\n",
    "        text = ''.join(text)\n",
    "        text = str(text)\n",
    "        \n",
    "        for tag in html_tags:\n",
    "            text = text.replace(tag,' ')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def abbrev_clean(self, raw_text):\n",
    "        #This function takes a string as an input and returns a word list\n",
    "        #This function performs an Apostrophe Lookup on the words in the list\n",
    "        combined_text = str()\n",
    "        \n",
    "        combined_text = raw_text.replace(\"Can\\'t\", \"Can not\")\n",
    "        combined_text = combined_text.replace(\"can\\'t\", \"can not\")\n",
    "        combined_text = combined_text.replace(\"didn\\'t\", \"did not\")\n",
    "        combined_text = combined_text.replace(\"Didn\\'t\", \"did not\")\n",
    "        combined_text = combined_text.replace(\"Can \\'t\", \"Can not\")\n",
    "        combined_text = combined_text.replace(\"can \\'t\", \"can not\")\n",
    "        combined_text = combined_text.replace(\"didn \\'t\", \"did not\")\n",
    "        combined_text = combined_text.replace(\"Didn \\'t\", \"did not\")\n",
    "        combined_text = combined_text.replace(\"it\\'s\", \"it is\")\n",
    "        combined_text = combined_text.replace(\"It\\'s\", \"It is\")\n",
    "        combined_text = combined_text.replace(\"it \\'s\", \"it is\")\n",
    "        combined_text = combined_text.replace(\"It \\'s\", \"It is\")\n",
    "        combined_text = combined_text.replace(\"we\\'ve\", \"we have\")\n",
    "        combined_text = combined_text.replace(\"We\\'ve\", \"We have\")\n",
    "        combined_text = combined_text.replace(\"won\\'t\", \"will not\")\n",
    "        combined_text = combined_text.replace(\"Won\\'t\", \"Will not\")\n",
    "        combined_text = re.sub('\\\\bve\\\\b','have',combined_text)\n",
    "\n",
    "        return combined_text\n",
    "    \n",
    "    def stop_punc_clean(self, word_list):\n",
    "        #This function takes a word list as an input and returns a word list\n",
    "        #This function removes stop words and punctuation marks using the nltk module\n",
    "        word_list = word_list.split(' ')\n",
    "        StopWords = stopwords.words(\"english\")\n",
    "        word_list = [word for word in word_list if word not in StopWords]\n",
    "        \n",
    "        combined_list = ' '.join(word_list)\n",
    "        combined_list = combined_list.translate(None, string.punctuation)\n",
    "        word_list_clean = combined_list.split(' ')\n",
    "        word_list_clean = filter(None, word_list_clean)\n",
    "        return word_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_date_text = open('/Volumes/articles/test.html', 'r')\n",
    "cleaner = text_cleaner()\n",
    "raw_text = cleaner.clean_text_file(folder_date_text)\n",
    "combined_text = cleaner.abbrev_clean(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "data_dir = '/Volumes/articles/cnn/raw/'\n",
    "events = listdir(data_dir)\n",
    "sources = [source for source in listdir(data_dir[0:18]) if '.' not in source]\n",
    "word_dict = dict()\n",
    "onlyfiles = [f for f in listdir('/Volumes/articles/cnn/raw/20160723') if isfile(join('/Volumes/articles/cnn/raw/20160723', f))]\n",
    "\n",
    "for source in sources:\n",
    "    word_dict[source] = events\n",
    "for event in events:\n",
    "    source_files = [f for f in listdir(data_dir + str(event)) if isfile(join(data_dir + str(event), f))]\n",
    "    word_dict[source][events.index(event)] = source_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'breitbart': ['20160723',\n",
       "  '20160925',\n",
       "  '20161004',\n",
       "  '20161005',\n",
       "  '20161007',\n",
       "  '20161019',\n",
       "  '20161028'],\n",
       " 'cnn': ['20160723',\n",
       "  '20160925',\n",
       "  '20161004',\n",
       "  '20161005',\n",
       "  '20161007',\n",
       "  '20161019',\n",
       "  '20161028'],\n",
       " 'fox': ['20160723',\n",
       "  '20160925',\n",
       "  '20161004',\n",
       "  '20161005',\n",
       "  '20161007',\n",
       "  '20161019',\n",
       "  '20161028'],\n",
       " 'msnbc': ['20160723',\n",
       "  '20160925',\n",
       "  '20161004',\n",
       "  '20161005',\n",
       "  '20161007',\n",
       "  '20161019',\n",
       "  '20161028']}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Want: generate a dictionary of a dictionary where each entry of the overarching dictionary is an event date. Each event date will have a list of sources underneath it. In each source will be a word list containing the cleaned up words for all the articles from that source on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:**\n",
    "    > for each event:\n",
    "            word_dict[event] = [list of dates]\n",
    "            for each date in dates:\n",
    "                word_dict[event][date] = [word_list from all articles on that date]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
